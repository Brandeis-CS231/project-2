{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65715c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import main, model\n",
    "from dataset import SeqPairDataset\n",
    "from model import EncoderDecoder\n",
    "from tokenizer import Tokenizer\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34e655a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success: vocabulary built!\n",
      "\n",
      "Sentence tokenized!\n",
      "src: ['\"', 'ay', ',', 'that', \"'\", 's', 'as', 'plain', 'a', 'pike', '-', 'staff', ',\"', 'say', 'barbara', ';', 'what', 'else', 'do', 'she', 'mean', ',', 'think', 'you', '?']\n",
      "tgt: ['\"', 'ay', ',', 'that', \"'\", 's', 'as', 'plain', 'as', 'a', 'pike', '-', 'staff', ',\"', 'said', 'barbara', ';', '\"', 'but', 'what', 'else', 'did', 'she', 'mean', ',', 'think', 'you', '?']\n",
      "\n",
      "\n",
      "Special tokens added:\n",
      "[274, 0, 1, 2, 3, 4, 5, 6, 7, 8]...\n",
      "\n",
      "Sentence length after padding: 100\n",
      "Padding tokens added:\n",
      "...[276, 276, 276, 276, 276, 276, 276, 276, 276, 276]\n",
      "\n",
      "Special tokens added:\n",
      "[274, 0, 1, 2, 3, 4, 5, 6, 7, 6]...\n",
      "\n",
      "Sentence length after padding: 100\n",
      "Padding tokens added:\n",
      "...[276, 276, 276, 276, 276, 276, 276, 276, 276, 276]\n",
      "\n",
      "Decoder input last 3: tensor([276, 276, 276])\n",
      "Target last 3       : [276, 276, 276]\n",
      "Decoder input size  : 99\n",
      "\n",
      "Labels first 3: tensor([0, 1, 2])\n",
      "Target first 3: [274, 0, 1]\n",
      "Labels size   : 99\n",
      "\n",
      "New sample created:\n",
      "    enc_inp size: 100    dec_inp size: 99    labels size : 99\n",
      "    enc_inp type: <class 'torch.Tensor'>\n",
      "    dec_inp type: <class 'torch.Tensor'>\n",
      "    labels  type: <class 'torch.Tensor'>\n",
      "-----------------------------------------------\n",
      "\n",
      "\n",
      "Sentence tokenized!\n",
      "src: ['there', 'be', 'little', 'sympathy', 'be', 'spar', 'for', 'body', 'else', '.']\n",
      "tgt: ['there', 'was', 'little', 'sympathy', 'to', 'be', 'spared', 'for', 'any', 'body', 'else', '.']\n",
      "\n",
      "\n",
      "Special tokens added:\n",
      "[274, 27, 28, 29, 30, 28, 277, 31, 32, 17]...\n",
      "\n",
      "Sentence length after padding: 100\n",
      "Padding tokens added:\n",
      "...[276, 276, 276, 276, 276, 276, 276, 276, 276, 276]\n",
      "\n",
      "Special tokens added:\n",
      "[274, 27, 34, 29, 30, 35, 28, 277, 31, 277]...\n",
      "\n",
      "Sentence length after padding: 100\n",
      "Padding tokens added:\n",
      "...[276, 276, 276, 276, 276, 276, 276, 276, 276, 276]\n",
      "\n",
      "Decoder input last 3: tensor([276, 276, 276])\n",
      "Target last 3       : [276, 276, 276]\n",
      "Decoder input size  : 99\n",
      "\n",
      "Labels first 3: tensor([27, 34, 29])\n",
      "Target first 3: [274, 27, 34]\n",
      "Labels size   : 99\n",
      "\n",
      "New sample created:\n",
      "    enc_inp size: 100    dec_inp size: 99    labels size : 99\n",
      "    enc_inp type: <class 'torch.Tensor'>\n",
      "    dec_inp type: <class 'torch.Tensor'>\n",
      "    labels  type: <class 'torch.Tensor'>\n",
      "-----------------------------------------------\n",
      "\n",
      "\n",
      "Sentence tokenized!\n",
      "src: ['102', ':', '5', 'by', 'reason', 'of', 'voice', 'of', 'my', 'groan', 'my', 'bone', 'cleave', 'my', 'skin', '.']\n",
      "tgt: ['102', ':', '5', 'by', 'reason', 'of', 'the', 'voice', 'of', 'my', 'groaning', 'my', 'bones', 'cleave', 'to', 'my', 'skin', '.']\n",
      "\n",
      "\n",
      "Special tokens added:\n",
      "[274, 36, 37, 38, 39, 40, 41, 42, 41, 43]...\n",
      "\n",
      "Sentence length after padding: 100\n",
      "Padding tokens added:\n",
      "...[276, 276, 276, 276, 276, 276, 276, 276, 276, 276]\n",
      "\n",
      "Special tokens added:\n",
      "[274, 36, 37, 38, 39, 40, 41, 46, 42, 41]...\n",
      "\n",
      "Sentence length after padding: 100\n",
      "Padding tokens added:\n",
      "...[276, 276, 276, 276, 276, 276, 276, 276, 276, 276]\n",
      "\n",
      "Decoder input last 3: tensor([276, 276, 276])\n",
      "Target last 3       : [276, 276, 276]\n",
      "Decoder input size  : 99\n",
      "\n",
      "Labels first 3: tensor([36, 37, 38])\n",
      "Target first 3: [274, 36, 37]\n",
      "Labels size   : 99\n",
      "\n",
      "New sample created:\n",
      "    enc_inp size: 100    dec_inp size: 99    labels size : 99\n",
      "    enc_inp type: <class 'torch.Tensor'>\n",
      "    dec_inp type: <class 'torch.Tensor'>\n",
      "    labels  type: <class 'torch.Tensor'>\n",
      "-----------------------------------------------\n",
      "\n",
      "\n",
      "Sentence tokenized!\n",
      "src: ['3', ':', '16', 'unto', 'woman', 'he', 'say', ',', 'i', 'will', 'greatly', 'multiply', 'thy', 'sorrow', 'thy', 'conception', ';', 'in', 'sorrow', 'thou', 'shalt', 'bring', 'forth', 'child', ';', 'thy', 'desire', 'shall', 'be', 'thy', 'husband', ',', 'he', 'shall', 'rule', 'over', 'thee', '.']\n",
      "tgt: ['3', ':', '16', 'unto', 'the', 'woman', 'he', 'said', ',', 'i', 'will', 'greatly', 'multiply', 'thy', 'sorrow', 'and', 'thy', 'conception', ';', 'in', 'sorrow', 'thou', 'shalt', 'bring', 'forth', 'children', ';', 'and', 'thy', 'desire', 'shall', 'be', 'to', 'thy', 'husband', ',', 'and', 'he', 'shall', 'rule', 'over', 'thee', '.']\n",
      "\n",
      "\n",
      "Special tokens added:\n",
      "[274, 47, 37, 48, 49, 50, 51, 13, 2, 52]...\n",
      "\n",
      "Sentence length after padding: 100\n",
      "Padding tokens added:\n",
      "...[276, 276, 276, 276, 276, 276, 276, 276, 276, 276]\n",
      "\n",
      "Special tokens added:\n",
      "[274, 47, 37, 48, 49, 46, 50, 51, 24, 2]...\n",
      "\n",
      "Sentence length after padding: 100\n",
      "Padding tokens added:\n",
      "...[276, 276, 276, 276, 276, 276, 276, 276, 276, 276]\n",
      "\n",
      "Decoder input last 3: tensor([276, 276, 276])\n",
      "Target last 3       : [276, 276, 276]\n",
      "Decoder input size  : 99\n",
      "\n",
      "Labels first 3: tensor([47, 37, 48])\n",
      "Target first 3: [274, 47, 37]\n",
      "Labels size   : 99\n",
      "\n",
      "New sample created:\n",
      "    enc_inp size: 100    dec_inp size: 99    labels size : 99\n",
      "    enc_inp type: <class 'torch.Tensor'>\n",
      "    dec_inp type: <class 'torch.Tensor'>\n",
      "    labels  type: <class 'torch.Tensor'>\n",
      "-----------------------------------------------\n",
      "\n",
      "\n",
      "Sentence tokenized!\n",
      "src: ['there', \"'\", 's', 'sermon', 'now', ',', 'writ', 'in', 'high', 'heaven', ',', 'sun', 'go', 'through', 'it', 'year', ',', 'yet', 'come', 'out', 'of', 'it', 'alive', 'hearty', '.']\n",
      "tgt: ['there', \"'\", 's', 'a', 'sermon', 'now', ',', 'writ', 'in', 'high', 'heaven', ',', 'and', 'the', 'sun', 'goes', 'through', 'it', 'every', 'year', ',', 'and', 'yet', 'comes', 'out', 'of', 'it', 'all', 'alive', 'and', 'hearty', '.']\n",
      "\n",
      "\n",
      "Special tokens added:\n",
      "[274, 27, 4, 5, 71, 72, 2, 73, 59, 74]...\n",
      "\n",
      "Sentence length after padding: 100\n",
      "Padding tokens added:\n",
      "...[276, 276, 276, 276, 276, 276, 276, 276, 276, 276]\n",
      "\n",
      "Special tokens added:\n",
      "[274, 27, 4, 5, 8, 71, 72, 2, 73, 59]...\n",
      "\n",
      "Sentence length after padding: 100\n",
      "Padding tokens added:\n",
      "...[276, 276, 276, 276, 276, 276, 276, 276, 276, 276]\n",
      "\n",
      "Decoder input last 3: tensor([276, 276, 276])\n",
      "Target last 3       : [276, 276, 276]\n",
      "Decoder input size  : 99\n",
      "\n",
      "Labels first 3: tensor([27,  4,  5])\n",
      "Target first 3: [274, 27, 4]\n",
      "Labels size   : 99\n",
      "\n",
      "New sample created:\n",
      "    enc_inp size: 100    dec_inp size: 99    labels size : 99\n",
      "    enc_inp type: <class 'torch.Tensor'>\n",
      "    dec_inp type: <class 'torch.Tensor'>\n",
      "    labels  type: <class 'torch.Tensor'>\n",
      "-----------------------------------------------\n",
      "\n",
      "\n",
      "Sentence tokenized!\n",
      "src: ['i', 'tell', 'you', 'what', ',', 'i', \"'\", 've', 'be', 'plague', 'ever', 'since', 'i', 'be', 'awake', ',', 'before', 'i', 'be', 'awake', ',', 'about', 'old', 'man', '.']\n",
      "tgt: ['i', 'tell', 'you', 'what', ',', 'i', \"'\", 've', 'been', 'plagued', 'ever', 'since', 'i', 'was', 'awake', ',', 'and', 'before', 'i', 'was', 'awake', ',', 'about', 'that', 'old', 'man', '.']\n",
      "\n",
      "\n",
      "Special tokens added:\n",
      "[274, 52, 87, 22, 16, 2, 52, 4, 88, 28]...\n",
      "\n",
      "Sentence length after padding: 100\n",
      "Padding tokens added:\n",
      "...[276, 276, 276, 276, 276, 276, 276, 276, 276, 276]\n",
      "\n",
      "Special tokens added:\n",
      "[274, 52, 87, 22, 16, 2, 52, 4, 88, 96]...\n",
      "\n",
      "Sentence length after padding: 100\n",
      "Padding tokens added:\n",
      "...[276, 276, 276, 276, 276, 276, 276, 276, 276, 276]\n",
      "\n",
      "Decoder input last 3: tensor([276, 276, 276])\n",
      "Target last 3       : [276, 276, 276]\n",
      "Decoder input size  : 99\n",
      "\n",
      "Labels first 3: tensor([52, 87, 22])\n",
      "Target first 3: [274, 52, 87]\n",
      "Labels size   : 99\n",
      "\n",
      "New sample created:\n",
      "    enc_inp size: 100    dec_inp size: 99    labels size : 99\n",
      "    enc_inp type: <class 'torch.Tensor'>\n",
      "    dec_inp type: <class 'torch.Tensor'>\n",
      "    labels  type: <class 'torch.Tensor'>\n",
      "-----------------------------------------------\n",
      "\n",
      "\n",
      "Sentence tokenized!\n",
      "src: ['23', ':', '29', 'david', 'go', 'up', 'from', 'thence', ',', 'dwelt', 'in', 'strong', 'hold', 'at', 'engedi', '.']\n",
      "tgt: ['23', ':', '29', 'and', 'david', 'went', 'up', 'from', 'thence', ',', 'and', 'dwelt', 'in', 'strong', 'holds', 'at', 'engedi', '.']\n",
      "\n",
      "\n",
      "Special tokens added:\n",
      "[274, 97, 37, 98, 99, 77, 100, 101, 102, 2]...\n",
      "\n",
      "Sentence length after padding: 100\n",
      "Padding tokens added:\n",
      "...[276, 276, 276, 276, 276, 276, 276, 276, 276, 276]\n",
      "\n",
      "Special tokens added:\n",
      "[274, 97, 37, 98, 70, 99, 277, 100, 101, 102]...\n",
      "\n",
      "Sentence length after padding: 100\n",
      "Padding tokens added:\n",
      "...[276, 276, 276, 276, 276, 276, 276, 276, 276, 276]\n",
      "\n",
      "Decoder input last 3: tensor([276, 276, 276])\n",
      "Target last 3       : [276, 276, 276]\n",
      "Decoder input size  : 99\n",
      "\n",
      "Labels first 3: tensor([97, 37, 98])\n",
      "Target first 3: [274, 97, 37]\n",
      "Labels size   : 99\n",
      "\n",
      "New sample created:\n",
      "    enc_inp size: 100    dec_inp size: 99    labels size : 99\n",
      "    enc_inp type: <class 'torch.Tensor'>\n",
      "    dec_inp type: <class 'torch.Tensor'>\n",
      "    labels  type: <class 'torch.Tensor'>\n",
      "-----------------------------------------------\n",
      "\n",
      "\n",
      "Sentence tokenized!\n",
      "src: ['lull', \"'\", 'd', 'late', 'be', 'smoke', 'of', 'first', '-', 'day', 'morning', ',', 'it', 'hang', 'low', 'over', 'row', 'of', 'tree', 'by', 'fence', ',', 'it', 'hang', 'thin', 'by', 'sassafras', 'wild', '-', 'cherry', 'cat', '-', 'brier', 'under', 'them', '.']\n",
      "tgt: ['lull', \"'\", 'd', 'and', 'late', 'is', 'the', 'smoke', 'of', 'the', 'first', '-', 'day', 'morning', ',', 'it', 'hangs', 'low', 'over', 'the', 'rows', 'of', 'trees', 'by', 'the', 'fences', ',', 'it', 'hangs', 'thin', 'by', 'the', 'sassafras', 'and', 'wild', '-', 'cherry', 'and', 'cat', '-', 'brier', 'under', 'them', '.']\n",
      "\n",
      "\n",
      "Special tokens added:\n",
      "[274, 108, 4, 109, 110, 28, 111, 41, 112, 10]...\n",
      "\n",
      "Sentence length after padding: 100\n",
      "Padding tokens added:\n",
      "...[276, 276, 276, 276, 276, 276, 276, 276, 276, 276]\n",
      "\n",
      "Special tokens added:\n",
      "[274, 108, 4, 109, 70, 110, 125, 46, 111, 41]...\n",
      "\n",
      "Sentence length after padding: 100\n",
      "Padding tokens added:\n",
      "...[276, 276, 276, 276, 276, 276, 276, 276, 276, 276]\n",
      "\n",
      "Decoder input last 3: tensor([276, 276, 276])\n",
      "Target last 3       : [276, 276, 276]\n",
      "Decoder input size  : 99\n",
      "\n",
      "Labels first 3: tensor([108,   4, 109])\n",
      "Target first 3: [274, 108, 4]\n",
      "Labels size   : 99\n",
      "\n",
      "New sample created:\n",
      "    enc_inp size: 100    dec_inp size: 99    labels size : 99\n",
      "    enc_inp type: <class 'torch.Tensor'>\n",
      "    dec_inp type: <class 'torch.Tensor'>\n",
      "    labels  type: <class 'torch.Tensor'>\n",
      "-----------------------------------------------\n",
      "\n",
      "\n",
      "Sentence tokenized!\n",
      "src: ['she', 'think', 'he', 'be', 'often', 'look', 'at', 'her', ',', 'try', 'for', 'full', 'view', 'of', 'her', 'face', 'than', 'it', 'suit', 'her', 'give', '.']\n",
      "tgt: ['she', 'thought', 'he', 'was', 'often', 'looking', 'at', 'her', ',', 'and', 'trying', 'for', 'a', 'fuller', 'view', 'of', 'her', 'face', 'than', 'it', 'suited', 'her', 'to', 'give', '.']\n",
      "\n",
      "\n",
      "Special tokens added:\n",
      "[274, 19, 21, 51, 28, 127, 128, 106, 129, 2]...\n",
      "\n",
      "Sentence length after padding: 100\n",
      "Padding tokens added:\n",
      "...[276, 276, 276, 276, 276, 276, 276, 276, 276, 276]\n",
      "\n",
      "Special tokens added:\n",
      "[274, 19, 277, 51, 34, 127, 277, 106, 129, 2]...\n",
      "\n",
      "Sentence length after padding: 100\n",
      "Padding tokens added:\n",
      "...[276, 276, 276, 276, 276, 276, 276, 276, 276, 276]\n",
      "\n",
      "Decoder input last 3: tensor([276, 276, 276])\n",
      "Target last 3       : [276, 276, 276]\n",
      "Decoder input size  : 99\n",
      "\n",
      "Labels first 3: tensor([ 19, 277,  51])\n",
      "Target first 3: [274, 19, 277]\n",
      "Labels size   : 99\n",
      "\n",
      "New sample created:\n",
      "    enc_inp size: 100    dec_inp size: 99    labels size : 99\n",
      "    enc_inp type: <class 'torch.Tensor'>\n",
      "    dec_inp type: <class 'torch.Tensor'>\n",
      "    labels  type: <class 'torch.Tensor'>\n",
      "-----------------------------------------------\n",
      "\n",
      "\n",
      "Sentence tokenized!\n",
      "src: ['42', ':', '31', 'we', 'say', 'unto', 'him', ',', 'we', 'be', 'true', 'men', ';', 'we', 'be', 'spy', ':', '42', ':', '32', 'we', 'be', 'twelve', 'brother', ',', 'son', 'of', 'our', 'father', ';', 'one', 'be', 'not', ',', 'young', 'be', 'day', 'with', 'our', 'father', 'in', 'land', 'of', 'canaan', '.']\n",
      "tgt: ['42', ':', '31', 'and', 'we', 'said', 'unto', 'him', ',', 'we', 'are', 'true', 'men', ';', 'we', 'are', 'no', 'spies', ':', '42', ':', '32', 'we', 'be', 'twelve', 'brethren', ',', 'sons', 'of', 'our', 'father', ';', 'one', 'is', 'not', ',', 'and', 'the', 'youngest', 'is', 'this', 'day', 'with', 'our', 'father', 'in', 'the', 'land', 'of', 'canaan', '.']\n",
      "\n",
      "\n",
      "Special tokens added:\n",
      "[274, 134, 37, 135, 136, 13, 49, 137, 2, 136]...\n",
      "\n",
      "Sentence length after padding: 100\n",
      "Padding tokens added:\n",
      "...[276, 276, 276, 276, 276, 276, 276, 276, 276, 276]\n",
      "\n",
      "Special tokens added:\n",
      "[274, 134, 37, 135, 70, 136, 24, 49, 137, 2]...\n",
      "\n",
      "Sentence length after padding: 100\n",
      "Padding tokens added:\n",
      "...[276, 276, 276, 276, 276, 276, 276, 276, 276, 276]\n",
      "\n",
      "Decoder input last 3: tensor([276, 276, 276])\n",
      "Target last 3       : [276, 276, 276]\n",
      "Decoder input size  : 99\n",
      "\n",
      "Labels first 3: tensor([134,  37, 135])\n",
      "Target first 3: [274, 134, 37]\n",
      "Labels size   : 99\n",
      "\n",
      "New sample created:\n",
      "    enc_inp size: 100    dec_inp size: 99    labels size : 99\n",
      "    enc_inp type: <class 'torch.Tensor'>\n",
      "    dec_inp type: <class 'torch.Tensor'>\n",
      "    labels  type: <class 'torch.Tensor'>\n",
      "-----------------------------------------------\n",
      "\n",
      "\n",
      "Sentence tokenized!\n",
      "src: ['somehow', 'i', 'grow', 'merry', 'again', '.']\n",
      "tgt: ['but', 'somehow', 'i', 'grew', 'merry', 'again', '.']\n",
      "\n",
      "\n",
      "Special tokens added:\n",
      "[274, 153, 52, 277, 154, 155, 33, 275]...\n",
      "\n",
      "Sentence length after padding: 100\n",
      "Padding tokens added:\n",
      "...[276, 276, 276, 276, 276, 276, 276, 276, 276, 276]\n",
      "\n",
      "Special tokens added:\n",
      "[274, 25, 153, 52, 277, 154, 155, 33, 275]...\n",
      "\n",
      "Sentence length after padding: 100\n",
      "Padding tokens added:\n",
      "...[276, 276, 276, 276, 276, 276, 276, 276, 276, 276]\n",
      "\n",
      "Decoder input last 3: tensor([276, 276, 276])\n",
      "Target last 3       : [276, 276, 276]\n",
      "Decoder input size  : 99\n",
      "\n",
      "Labels first 3: tensor([ 25, 153,  52])\n",
      "Target first 3: [274, 25, 153]\n",
      "Labels size   : 99\n",
      "\n",
      "New sample created:\n",
      "    enc_inp size: 100    dec_inp size: 99    labels size : 99\n",
      "    enc_inp type: <class 'torch.Tensor'>\n",
      "    dec_inp type: <class 'torch.Tensor'>\n",
      "    labels  type: <class 'torch.Tensor'>\n",
      "-----------------------------------------------\n",
      "\n",
      "\n",
      "Sentence tokenized!\n",
      "src: ['do', 'it', 'go', 'far', '?']\n",
      "tgt: ['does', 'it', 'go', 'further', '?']\n",
      "\n",
      "\n",
      "Special tokens added:\n",
      "[274, 18, 79, 77, 277, 23, 275]...\n",
      "\n",
      "Sentence length after padding: 100\n",
      "Padding tokens added:\n",
      "...[276, 276, 276, 276, 276, 276, 276, 276, 276, 276]\n",
      "\n",
      "Special tokens added:\n",
      "[274, 277, 79, 77, 277, 23, 275]...\n",
      "\n",
      "Sentence length after padding: 100\n",
      "Padding tokens added:\n",
      "...[276, 276, 276, 276, 276, 276, 276, 276, 276, 276]\n",
      "\n",
      "Decoder input last 3: tensor([276, 276, 276])\n",
      "Target last 3       : [276, 276, 276]\n",
      "Decoder input size  : 99\n",
      "\n",
      "Labels first 3: tensor([277,  79,  77])\n",
      "Target first 3: [274, 277, 79]\n",
      "Labels size   : 99\n",
      "\n",
      "New sample created:\n",
      "    enc_inp size: 100    dec_inp size: 99    labels size : 99\n",
      "    enc_inp type: <class 'torch.Tensor'>\n",
      "    dec_inp type: <class 'torch.Tensor'>\n",
      "    labels  type: <class 'torch.Tensor'>\n",
      "-----------------------------------------------\n",
      "\n",
      "\n",
      "Sentence tokenized!\n",
      "src: ['mere', 'name', 'strike', 'syme', 'cold', 'serious', ';', 'his', 'laughter', 'have', 'die', 'in', 'his', 'heart', 'before', 'it', 'could', 'die', 'on', 'his', 'lip', '.']\n",
      "tgt: ['and', 'the', 'mere', 'name', 'struck', 'syme', 'cold', 'and', 'serious', ';', 'his', 'laughter', 'had', 'died', 'in', 'his', 'heart', 'before', 'it', 'could', 'die', 'on', 'his', 'lips', '.']\n",
      "\n",
      "\n",
      "Special tokens added:\n",
      "[274, 156, 157, 277, 158, 159, 160, 15, 161, 162]...\n",
      "\n",
      "Sentence length after padding: 100\n",
      "Padding tokens added:\n",
      "...[276, 276, 276, 276, 276, 276, 276, 276, 276, 276]\n",
      "\n",
      "Special tokens added:\n",
      "[274, 70, 46, 156, 157, 277, 158, 159, 70, 160]...\n",
      "\n",
      "Sentence length after padding: 100\n",
      "Padding tokens added:\n",
      "...[276, 276, 276, 276, 276, 276, 276, 276, 276, 276]\n",
      "\n",
      "Decoder input last 3: tensor([276, 276, 276])\n",
      "Target last 3       : [276, 276, 276]\n",
      "Decoder input size  : 99\n",
      "\n",
      "Labels first 3: tensor([ 70,  46, 156])\n",
      "Target first 3: [274, 70, 46]\n",
      "Labels size   : 99\n",
      "\n",
      "New sample created:\n",
      "    enc_inp size: 100    dec_inp size: 99    labels size : 99\n",
      "    enc_inp type: <class 'torch.Tensor'>\n",
      "    dec_inp type: <class 'torch.Tensor'>\n",
      "    labels  type: <class 'torch.Tensor'>\n",
      "-----------------------------------------------\n",
      "\n",
      "\n",
      "Sentence tokenized!\n",
      "src: ['cecilia', 'tremble', 'so', 'that', 'she', 'could', 'not', 'hold', 'it', '.']\n",
      "tgt: ['cecilia', 'trembled', 'so', 'that', 'she', 'could', 'not', 'hold', 'it', '.']\n",
      "\n",
      "\n",
      "Special tokens added:\n",
      "[274, 170, 277, 171, 3, 19, 166, 146, 105, 79]...\n",
      "\n",
      "Sentence length after padding: 100\n",
      "Padding tokens added:\n",
      "...[276, 276, 276, 276, 276, 276, 276, 276, 276, 276]\n",
      "\n",
      "Special tokens added:\n",
      "[274, 170, 277, 171, 3, 19, 166, 146, 105, 79]...\n",
      "\n",
      "Sentence length after padding: 100\n",
      "Padding tokens added:\n",
      "...[276, 276, 276, 276, 276, 276, 276, 276, 276, 276]\n",
      "\n",
      "Decoder input last 3: tensor([276, 276, 276])\n",
      "Target last 3       : [276, 276, 276]\n",
      "Decoder input size  : 99\n",
      "\n",
      "Labels first 3: tensor([170, 277, 171])\n",
      "Target first 3: [274, 170, 277]\n",
      "Labels size   : 99\n",
      "\n",
      "New sample created:\n",
      "    enc_inp size: 100    dec_inp size: 99    labels size : 99\n",
      "    enc_inp type: <class 'torch.Tensor'>\n",
      "    dec_inp type: <class 'torch.Tensor'>\n",
      "    labels  type: <class 'torch.Tensor'>\n",
      "-----------------------------------------------\n",
      "\n",
      "\n",
      "Sentence tokenized!\n",
      "src: ['who', 'can', 'in', 'reason', 'then', ',', 'right', ',', 'assume', 'monarchy', 'over', 'such', 'a', 'live', 'by', 'right', 'his', 'equal', ',', 'if', 'in', 'power', 'splendour', 'less', ',', 'in', 'freedom', 'equal', '?']\n",
      "tgt: ['who', 'can', 'in', 'reason', 'then', ',', 'or', 'right', ',', 'assume', 'monarchy', 'over', 'such', 'as', 'live', 'by', 'right', 'his', 'equals', ',', 'if', 'in', 'power', 'and', 'splendour', 'less', ',', 'in', 'freedom', 'equal', '?']\n",
      "\n",
      "\n",
      "Special tokens added:\n",
      "[274, 172, 173, 59, 40, 174, 2, 175, 2, 176]...\n",
      "\n",
      "Sentence length after padding: 100\n",
      "Padding tokens added:\n",
      "...[276, 276, 276, 276, 276, 276, 276, 276, 276, 276]\n",
      "\n",
      "Special tokens added:\n",
      "[274, 172, 173, 59, 40, 174, 2, 277, 175, 2]...\n",
      "\n",
      "Sentence length after padding: 100\n",
      "Padding tokens added:\n",
      "...[276, 276, 276, 276, 276, 276, 276, 276, 276, 276]\n",
      "\n",
      "Decoder input last 3: tensor([276, 276, 276])\n",
      "Target last 3       : [276, 276, 276]\n",
      "Decoder input size  : 99\n",
      "\n",
      "Labels first 3: tensor([172, 173,  59])\n",
      "Target first 3: [274, 172, 173]\n",
      "Labels size   : 99\n",
      "\n",
      "New sample created:\n",
      "    enc_inp size: 100    dec_inp size: 99    labels size : 99\n",
      "    enc_inp type: <class 'torch.Tensor'>\n",
      "    dec_inp type: <class 'torch.Tensor'>\n",
      "    labels  type: <class 'torch.Tensor'>\n",
      "-----------------------------------------------\n",
      "\n",
      "\n",
      "Sentence tokenized!\n",
      "src: ['moreouer', ',', 'he', 'hath', 'leave', 'you', 'all', 'his', 'walkes', ',', 'his', 'priuate', 'arbor', ',', 'new', '-', 'plant', 'orchard', ',', 'on', 'side', 'tyber', ',', 'he', 'hath', 'leave', 'them', 'you', ',', 'your', 'heyres', 'for', 'euer', ':', 'common', 'pleasure', 'walke', 'abroad', ',', 'recreate', 'your', 'selues', '.']\n",
      "tgt: ['moreouer', ',', 'he', 'hath', 'left', 'you', 'all', 'his', 'walkes', ',', 'his', 'priuate', 'arbors', ',', 'and', 'new', '-', 'planted', 'orchards', ',', 'on', 'this', 'side', 'tyber', ',', 'he', 'hath', 'left', 'them', 'you', ',', 'and', 'to', 'your', 'heyres', 'for', 'euer', ':', 'common', 'pleasures', 'to', 'walke', 'abroad', ',', 'and', 'recreate', 'your', 'selues', '.']\n",
      "\n",
      "\n",
      "Special tokens added:\n",
      "[274, 186, 2, 51, 187, 188, 22, 86, 161, 189]...\n",
      "\n",
      "Sentence length after padding: 100\n",
      "Padding tokens added:\n",
      "...[276, 276, 276, 276, 276, 276, 276, 276, 276, 276]\n",
      "\n",
      "Special tokens added:\n",
      "[274, 186, 2, 51, 187, 202, 22, 86, 161, 189]...\n",
      "\n",
      "Sentence length after padding: 100\n",
      "Padding tokens added:\n",
      "...[276, 276, 276, 276, 276, 276, 276, 276, 276, 276]\n",
      "\n",
      "Decoder input last 3: tensor([276, 276, 276])\n",
      "Target last 3       : [276, 276, 276]\n",
      "Decoder input size  : 99\n",
      "\n",
      "Labels first 3: tensor([186,   2,  51])\n",
      "Target first 3: [274, 186, 2]\n",
      "Labels size   : 99\n",
      "\n",
      "New sample created:\n",
      "    enc_inp size: 100    dec_inp size: 99    labels size : 99\n",
      "    enc_inp type: <class 'torch.Tensor'>\n",
      "    dec_inp type: <class 'torch.Tensor'>\n",
      "    labels  type: <class 'torch.Tensor'>\n",
      "-----------------------------------------------\n",
      "\n",
      "\n",
      "Sentence tokenized!\n",
      "src: ['24', ':', '4', 'he', 'that', 'hath', 'clean', 'hand', ',', 'pure', 'heart', ';', 'who', 'hath', 'not', 'lift', 'up', 'his', 'soul', 'unto', 'vanity', ',', 'sworn', 'deceitfully', '.']\n",
      "tgt: ['24', ':', '4', 'he', 'that', 'hath', 'clean', 'hands', ',', 'and', 'a', 'pure', 'heart', ';', 'who', 'hath', 'not', 'lifted', 'up', 'his', 'soul', 'unto', 'vanity', ',', 'nor', 'sworn', 'deceitfully', '.']\n",
      "\n",
      "\n",
      "Special tokens added:\n",
      "[274, 203, 37, 204, 51, 3, 187, 205, 277, 2]...\n",
      "\n",
      "Sentence length after padding: 100\n",
      "Padding tokens added:\n",
      "...[276, 276, 276, 276, 276, 276, 276, 276, 276, 276]\n",
      "\n",
      "Special tokens added:\n",
      "[274, 203, 37, 204, 51, 3, 187, 205, 277, 2]...\n",
      "\n",
      "Sentence length after padding: 100\n",
      "Padding tokens added:\n",
      "...[276, 276, 276, 276, 276, 276, 276, 276, 276, 276]\n",
      "\n",
      "Decoder input last 3: tensor([276, 276, 276])\n",
      "Target last 3       : [276, 276, 276]\n",
      "Decoder input size  : 99\n",
      "\n",
      "Labels first 3: tensor([203,  37, 204])\n",
      "Target first 3: [274, 203, 37]\n",
      "Labels size   : 99\n",
      "\n",
      "New sample created:\n",
      "    enc_inp size: 100    dec_inp size: 99    labels size : 99\n",
      "    enc_inp type: <class 'torch.Tensor'>\n",
      "    dec_inp type: <class 'torch.Tensor'>\n",
      "    labels  type: <class 'torch.Tensor'>\n",
      "-----------------------------------------------\n",
      "\n",
      "\n",
      "Sentence tokenized!\n",
      "src: ['i', 'know', 'what', 'thorough', 'justice', 'you', 'will', 'do', 'it', ',', 'have', 'scarcely', 'doubt', 'of', 'it', 'happy', 'effect', '.--', 'i', 'think', 'we', 'shall', 'never', 'materially', 'disagree', 'about', 'writer', 'again', ';', 'i', 'will', 'not', 'delay', 'you', 'by', 'long', 'preface', '.--', 'we', 'be', 'quite', 'well', '.--', 'letter', 'have', 'be', 'cure', 'of', 'all', 'little', 'nervousness', 'i', 'have', 'be', 'feel', 'lately', '.--', 'i', 'do', 'not', 'quite', 'like', 'your', 'look', 'on', 'tuesday', ',', 'it', 'be', 'ungenial', 'morning', ';', 'though', 'you', 'will', 'never', 'own', 'be', 'affect', 'by', 'weather', ',', 'i', 'think', 'body', 'feel', 'north', '-', 'east', 'wind', '.--', 'i', 'felt', 'for', 'your', 'dear', 'father', 'very', 'much', 'in', 'storm', 'of', 'tuesday', 'afternoon', 'yesterday', 'morning', ',', 'have', 'comfort', 'of', 'hear', 'last', 'night', ',', 'by', 'mr', '.', 'perry', ',', 'that', 'it', 'have', 'not', 'make', 'him', 'ill', '.', '\"', 'yours', 'ever', ',', '\"', 'a', '.', 'w', '.\"']\n",
      "tgt: ['i', 'know', 'what', 'thorough', 'justice', 'you', 'will', 'do', 'it', ',', 'and', 'have', 'scarcely', 'a', 'doubt', 'of', 'its', 'happy', 'effect', '.--', 'i', 'think', 'we', 'shall', 'never', 'materially', 'disagree', 'about', 'the', 'writer', 'again', ';', 'but', 'i', 'will', 'not', 'delay', 'you', 'by', 'a', 'long', 'preface', '.--', 'we', 'are', 'quite', 'well', '.--', 'this', 'letter', 'has', 'been', 'the', 'cure', 'of', 'all', 'the', 'little', 'nervousness', 'i', 'have', 'been', 'feeling', 'lately', '.--', 'i', 'did', 'not', 'quite', 'like', 'your', 'looks', 'on', 'tuesday', ',', 'but', 'it', 'was', 'an', 'ungenial', 'morning', ';', 'and', 'though', 'you', 'will', 'never', 'own', 'being', 'affected', 'by', 'weather', ',', 'i', 'think', 'every', 'body', 'feels', 'a', 'north', '-', 'east', 'wind', '.--', 'i', 'felt', 'for', 'your', 'dear', 'father', 'very', 'much', 'in', 'the', 'storm', 'of', 'tuesday', 'afternoon', 'and', 'yesterday', 'morning', ',', 'but', 'had', 'the', 'comfort', 'of', 'hearing', 'last', 'night', ',', 'by', 'mr', '.', 'perry', ',', 'that', 'it', 'had', 'not', 'made', 'him', 'ill', '.', '\"', 'yours', 'ever', ',', '\"', 'a', '.', 'w', '.\"']\n",
      "\n",
      "\n",
      "Excessive sentence trimmed to: 98 prior to adding <BOS> and <EOS>\n",
      "Special tokens added:\n",
      "[274, 52, 211, 16, 212, 213, 22, 53, 18, 79]...\n",
      "\n",
      "Sentence length after padding: 100\n",
      "Padding tokens added:\n",
      "...[241, 218, 52, 242, 31, 194, 243, 144, 244, 275]\n",
      "\n",
      "Excessive sentence trimmed to: 98 prior to adding <BOS> and <EOS>\n",
      "Special tokens added:\n",
      "[274, 52, 211, 16, 212, 213, 22, 53, 18, 79]...\n",
      "\n",
      "Sentence length after padding: 100\n",
      "Padding tokens added:\n",
      "...[277, 39, 238, 2, 52, 21, 85, 32, 277, 275]\n",
      "\n",
      "Decoder input last 3: tensor([ 85,  32, 277])\n",
      "Target last 3       : [32, 277, 275]\n",
      "Decoder input size  : 99\n",
      "\n",
      "Labels first 3: tensor([ 52, 211,  16])\n",
      "Target first 3: [274, 52, 211]\n",
      "Labels size   : 99\n",
      "\n",
      "New sample created:\n",
      "    enc_inp size: 100    dec_inp size: 99    labels size : 99\n",
      "    enc_inp type: <class 'torch.Tensor'>\n",
      "    dec_inp type: <class 'torch.Tensor'>\n",
      "    labels  type: <class 'torch.Tensor'>\n",
      "-----------------------------------------------\n",
      "\n",
      "\n",
      "Sentence tokenized!\n",
      "src: ['5', ':', '18', 'jared', 'live', 'hundred', 'sixty', 'two', 'year', ',', 'he', 'beget', 'enoch', ':', '5', ':', '19', 'jared', 'live', 'after', 'he', 'beget', 'enoch', 'eight', 'hundred', 'year', ',', 'begat', 'son', 'daughter', ':', '5', ':', '20', 'all', 'day', 'of', 'jared', 'be', 'nine', 'hundred', 'sixty', 'two', 'year', ':', 'he', 'die', '.']\n",
      "tgt: ['5', ':', '18', 'and', 'jared', 'lived', 'an', 'hundred', 'sixty', 'and', 'two', 'years', ',', 'and', 'he', 'begat', 'enoch', ':', '5', ':', '19', 'and', 'jared', 'lived', 'after', 'he', 'begat', 'enoch', 'eight', 'hundred', 'years', ',', 'and', 'begat', 'sons', 'and', 'daughters', ':', '5', ':', '20', 'and', 'all', 'the', 'days', 'of', 'jared', 'were', 'nine', 'hundred', 'sixty', 'and', 'two', 'years', ':', 'and', 'he', 'died', '.']\n",
      "\n",
      "\n",
      "Special tokens added:\n",
      "[274, 38, 37, 259, 260, 179, 261, 262, 263, 80]...\n",
      "\n",
      "Sentence length after padding: 100\n",
      "Padding tokens added:\n",
      "...[276, 276, 276, 276, 276, 276, 276, 276, 276, 276]\n",
      "\n",
      "Special tokens added:\n",
      "[274, 38, 37, 259, 70, 260, 272, 258, 261, 262]...\n",
      "\n",
      "Sentence length after padding: 100\n",
      "Padding tokens added:\n",
      "...[276, 276, 276, 276, 276, 276, 276, 276, 276, 276]\n",
      "\n",
      "Decoder input last 3: tensor([276, 276, 276])\n",
      "Target last 3       : [276, 276, 276]\n",
      "Decoder input size  : 99\n",
      "\n",
      "Labels first 3: tensor([ 38,  37, 259])\n",
      "Target first 3: [274, 38, 37]\n",
      "Labels size   : 99\n",
      "\n",
      "New sample created:\n",
      "    enc_inp size: 100    dec_inp size: 99    labels size : 99\n",
      "    enc_inp type: <class 'torch.Tensor'>\n",
      "    dec_inp type: <class 'torch.Tensor'>\n",
      "    labels  type: <class 'torch.Tensor'>\n",
      "-----------------------------------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "MAX_LEN = 100\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "# build vocabulary and token_id dicts\n",
    "tokenizer.from_file(\"data/toy.json\")\n",
    "tr_data = SeqPairDataset(\"data/toy.json\", tokenizer, MAX_LEN, MAX_LEN)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb2d119c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(tr_data, 64, True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ec65c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_vocab_size = len(tokenizer.src_vocab)\n",
    "tgt_vocab_size = len(tokenizer.tgt_vocab)\n",
    "pad_id = tokenizer.pad_id\n",
    "\n",
    "VOCAB_SIZE = len(tokenizer.src_vocab)\n",
    "\n",
    "model = EncoderDecoder(src_vocab_size=VOCAB_SIZE, \n",
    "    tgt_vocab_size=VOCAB_SIZE,\n",
    "    pad_idx=pad_id, \n",
    "    max_len=MAX_LEN\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1deadc9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=pad_id)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "330fc6ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1/1 [00:00<00:00,  6.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0 processed!    19 samples trained\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "loss = main.train_epoch(\n",
    "    model,\n",
    "    dataloader,\n",
    "    optimizer,\n",
    "    loss_fn\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22996758",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success: vocabulary built!\n",
      "\n",
      "Sentence tokenized!\n",
      "src: ['you', 'gingerly', 'rascal', '!']\n",
      "tgt: ['you', 'gingerly', 'rascal', '!']\n",
      "\n",
      "\n",
      "Special tokens added:\n",
      "[274, 22, 277, 277, 277, 275]...\n",
      "\n",
      "Sentence length after padding: 100\n",
      "Padding tokens added:\n",
      "...[276, 276, 276, 276, 276, 276, 276, 276, 276, 276]\n",
      "\n",
      "Special tokens added:\n",
      "[274, 22, 277, 277, 277, 275]...\n",
      "\n",
      "Sentence length after padding: 100\n",
      "Padding tokens added:\n",
      "...[276, 276, 276, 276, 276, 276, 276, 276, 276, 276]\n",
      "\n",
      "Decoder input last 3: tensor([276, 276, 276])\n",
      "Target last 3       : [276, 276, 276]\n",
      "Decoder input size  : 99\n",
      "\n",
      "Labels first 3: tensor([ 22, 277, 277])\n",
      "Target first 3: [274, 22, 277]\n",
      "Labels size   : 99\n",
      "\n",
      "New sample created:\n",
      "    enc_inp size: 100    dec_inp size: 99    labels size : 99\n",
      "    enc_inp type: <class 'torch.Tensor'>\n",
      "    dec_inp type: <class 'torch.Tensor'>\n",
      "    labels  type: <class 'torch.Tensor'>\n",
      "-----------------------------------------------\n",
      "\n",
      "\n",
      "Sentence tokenized!\n",
      "src: ['miss', 'steele', 'be', 'least', 'discomposed', 'of', 'three', ',', 'by', 'their', 'presence', ';', 'it', 'be', 'in', 'their', 'power', 'reconcile', 'her', 'it', 'entirely', '.']\n",
      "tgt: ['miss', 'steele', 'was', 'the', 'least', 'discomposed', 'of', 'the', 'three', ',', 'by', 'their', 'presence', ';', 'and', 'it', 'was', 'in', 'their', 'power', 'to', 'reconcile', 'her', 'to', 'it', 'entirely', '.']\n",
      "\n",
      "\n",
      "Special tokens added:\n",
      "[274, 277, 277, 28, 277, 277, 41, 277, 2, 39]...\n",
      "\n",
      "Sentence length after padding: 100\n",
      "Padding tokens added:\n",
      "...[276, 276, 276, 276, 276, 276, 276, 276, 276, 276]\n",
      "\n",
      "Special tokens added:\n",
      "[274, 277, 277, 34, 46, 277, 277, 41, 46, 277]...\n",
      "\n",
      "Sentence length after padding: 100\n",
      "Padding tokens added:\n",
      "...[276, 276, 276, 276, 276, 276, 276, 276, 276, 276]\n",
      "\n",
      "Decoder input last 3: tensor([276, 276, 276])\n",
      "Target last 3       : [276, 276, 276]\n",
      "Decoder input size  : 99\n",
      "\n",
      "Labels first 3: tensor([277, 277,  34])\n",
      "Target first 3: [274, 277, 277]\n",
      "Labels size   : 99\n",
      "\n",
      "New sample created:\n",
      "    enc_inp size: 100    dec_inp size: 99    labels size : 99\n",
      "    enc_inp type: <class 'torch.Tensor'>\n",
      "    dec_inp type: <class 'torch.Tensor'>\n",
      "    labels  type: <class 'torch.Tensor'>\n",
      "-----------------------------------------------\n",
      "\n",
      "\n",
      "Sentence tokenized!\n",
      "src: ['turnbull', 'macian', 'look', 'at', 'other', ',', 'say', 'more', 'than', 'they', 'could', 'ever', 'say', 'with', 'tongue', 'sword', '.']\n",
      "tgt: ['turnbull', 'and', 'macian', 'looked', 'at', 'each', 'other', ',', 'and', 'said', 'more', 'than', 'they', 'could', 'ever', 'say', 'with', 'tongues', 'or', 'swords', '.']\n",
      "\n",
      "\n",
      "Special tokens added:\n",
      "[274, 277, 277, 128, 106, 277, 2, 13, 277, 132]...\n",
      "\n",
      "Sentence length after padding: 100\n",
      "Padding tokens added:\n",
      "...[276, 276, 276, 276, 276, 276, 276, 276, 276, 276]\n",
      "\n",
      "Special tokens added:\n",
      "[274, 277, 70, 277, 277, 106, 277, 277, 2, 70]...\n",
      "\n",
      "Sentence length after padding: 100\n",
      "Padding tokens added:\n",
      "...[276, 276, 276, 276, 276, 276, 276, 276, 276, 276]\n",
      "\n",
      "Decoder input last 3: tensor([276, 276, 276])\n",
      "Target last 3       : [276, 276, 276]\n",
      "Decoder input size  : 99\n",
      "\n",
      "Labels first 3: tensor([277,  70, 277])\n",
      "Target first 3: [274, 277, 70]\n",
      "Labels size   : 99\n",
      "\n",
      "New sample created:\n",
      "    enc_inp size: 100    dec_inp size: 99    labels size : 99\n",
      "    enc_inp type: <class 'torch.Tensor'>\n",
      "    dec_inp type: <class 'torch.Tensor'>\n",
      "    labels  type: <class 'torch.Tensor'>\n",
      "-----------------------------------------------\n",
      "\n",
      "\n",
      "Sentence tokenized!\n",
      "src: ['3', ':', '1', 'i', ',', 'brother', ',', 'could', 'not', 'speak', 'unto', 'you', 'a', 'unto', 'spiritual', ',', 'a', 'unto', 'carnal', ',', 'even', 'a', 'unto', 'babe', 'in', 'christ', '.']\n",
      "tgt: ['3', ':', '1', 'and', 'i', ',', 'brethren', ',', 'could', 'not', 'speak', 'unto', 'you', 'as', 'unto', 'spiritual', ',', 'but', 'as', 'unto', 'carnal', ',', 'even', 'as', 'unto', 'babes', 'in', 'christ', '.']\n",
      "\n",
      "\n",
      "Special tokens added:\n",
      "[274, 47, 37, 277, 52, 2, 277, 2, 166, 146]...\n",
      "\n",
      "Sentence length after padding: 100\n",
      "Padding tokens added:\n",
      "...[276, 276, 276, 276, 276, 276, 276, 276, 276, 276]\n",
      "\n",
      "Special tokens added:\n",
      "[274, 47, 37, 277, 70, 52, 2, 277, 2, 166]...\n",
      "\n",
      "Sentence length after padding: 100\n",
      "Padding tokens added:\n",
      "...[276, 276, 276, 276, 276, 276, 276, 276, 276, 276]\n",
      "\n",
      "Decoder input last 3: tensor([276, 276, 276])\n",
      "Target last 3       : [276, 276, 276]\n",
      "Decoder input size  : 99\n",
      "\n",
      "Labels first 3: tensor([ 47,  37, 277])\n",
      "Target first 3: [274, 47, 37]\n",
      "Labels size   : 99\n",
      "\n",
      "New sample created:\n",
      "    enc_inp size: 100    dec_inp size: 99    labels size : 99\n",
      "    enc_inp type: <class 'torch.Tensor'>\n",
      "    dec_inp type: <class 'torch.Tensor'>\n",
      "    labels  type: <class 'torch.Tensor'>\n",
      "-----------------------------------------------\n",
      "\n",
      "\n",
      "Sentence tokenized!\n",
      "src: ['be', 'quite', 'female', 'case', ',', 'it', 'would', 'be', 'highly', 'absurd', 'in', 'him', ',', 'who', 'could', 'be', 'of', 'use', 'at', 'home', ',', 'shut', 'himself', 'up', '.']\n",
      "tgt: ['this', 'was', 'quite', 'a', 'female', 'case', ',', 'and', 'it', 'would', 'be', 'highly', 'absurd', 'in', 'him', ',', 'who', 'could', 'be', 'of', 'no', 'use', 'at', 'home', ',', 'to', 'shut', 'himself', 'up', '.']\n",
      "\n",
      "\n",
      "Special tokens added:\n",
      "[274, 28, 226, 277, 277, 2, 79, 277, 28, 277]...\n",
      "\n",
      "Sentence length after padding: 100\n",
      "Padding tokens added:\n",
      "...[276, 276, 276, 276, 276, 276, 276, 276, 276, 276]\n",
      "\n",
      "Special tokens added:\n",
      "[274, 152, 34, 226, 8, 277, 277, 2, 70, 79]...\n",
      "\n",
      "Sentence length after padding: 100\n",
      "Padding tokens added:\n",
      "...[276, 276, 276, 276, 276, 276, 276, 276, 276, 276]\n",
      "\n",
      "Decoder input last 3: tensor([276, 276, 276])\n",
      "Target last 3       : [276, 276, 276]\n",
      "Decoder input size  : 99\n",
      "\n",
      "Labels first 3: tensor([152,  34, 226])\n",
      "Target first 3: [274, 152, 34]\n",
      "Labels size   : 99\n",
      "\n",
      "New sample created:\n",
      "    enc_inp size: 100    dec_inp size: 99    labels size : 99\n",
      "    enc_inp type: <class 'torch.Tensor'>\n",
      "    dec_inp type: <class 'torch.Tensor'>\n",
      "    labels  type: <class 'torch.Tensor'>\n",
      "-----------------------------------------------\n",
      "\n",
      "\n",
      "Sentence tokenized!\n",
      "src: ['it', 'have', 'be', 'very', 'happy', 'fortnight', ',', 'forlorn', 'must', 'be', 'sinking', 'from', 'it', 'into', 'common', 'course', 'of', 'hartfield', 'day', '.']\n",
      "tgt: ['it', 'had', 'been', 'a', 'very', 'happy', 'fortnight', ',', 'and', 'forlorn', 'must', 'be', 'the', 'sinking', 'from', 'it', 'into', 'the', 'common', 'course', 'of', 'hartfield', 'days', '.']\n",
      "\n",
      "\n",
      "Special tokens added:\n",
      "[274, 79, 163, 28, 244, 216, 277, 2, 277, 277]...\n",
      "\n",
      "Sentence length after padding: 100\n",
      "Padding tokens added:\n",
      "...[276, 276, 276, 276, 276, 276, 276, 276, 276, 276]\n",
      "\n",
      "Special tokens added:\n",
      "[274, 79, 168, 96, 8, 244, 216, 277, 2, 70]...\n",
      "\n",
      "Sentence length after padding: 100\n",
      "Padding tokens added:\n",
      "...[276, 276, 276, 276, 276, 276, 276, 276, 276, 276]\n",
      "\n",
      "Decoder input last 3: tensor([276, 276, 276])\n",
      "Target last 3       : [276, 276, 276]\n",
      "Decoder input size  : 99\n",
      "\n",
      "Labels first 3: tensor([ 79, 168,  96])\n",
      "Target first 3: [274, 79, 168]\n",
      "Labels size   : 99\n",
      "\n",
      "New sample created:\n",
      "    enc_inp size: 100    dec_inp size: 99    labels size : 99\n",
      "    enc_inp type: <class 'torch.Tensor'>\n",
      "    dec_inp type: <class 'torch.Tensor'>\n",
      "    labels  type: <class 'torch.Tensor'>\n",
      "-----------------------------------------------\n",
      "\n",
      "\n",
      "Sentence tokenized!\n",
      "src: ['in', 'fact', ',', 'felix', 'have', 'till', 'now', 'profess', 'himself', 'his', 'firm', 'ally', ',', 'have', 'on', 'his', 'part', 'receive', 'from', 'franklin', 'unequivocal', 'proof', 'of', 'friendship', ';', 'for', 'it', 'must', 'be', 'tell', 'that', 'other', 'morning', ',', 'when', 'it', 'be', 'felix', \"'\", 's', 'turn', 'get', 'breakfast', ',', 'felix', 'never', 'be', 'up', 'in', 'decent', 'time', ',', 'must', 'inevitably', 'have', 'come', 'public', 'disgrace', 'if', 'franklin', 'have', 'not', 'get', 'all', 'breakfast', 'thing', 'ready', 'for', 'him', ',', 'bread', 'butter', 'spread', ',', 'toast', 'toast', ';', 'have', 'not', ',', 'moreover', ',', 'regularly', ',', 'when', 'clock', 'strike', 'eight', ',', 'mr', '.', 'pomfret', \"'\", 's', 'foot', 'be', 'hear', 'overhead', ',', 'run', 'call', 'sleeping', 'felix', ',', 'help', 'him', 'constantly', 'through', 'hurry', 'of', 'get', 'dress', 'one', 'instant', 'before', 'housekeeper', 'come', 'downstairs', '.']\n",
      "tgt: ['in', 'fact', ',', 'felix', 'had', 'till', 'now', 'professed', 'himself', 'his', 'firm', 'ally', ',', 'and', 'had', 'on', 'his', 'part', 'received', 'from', 'franklin', 'unequivocal', 'proofs', 'of', 'friendship', ';', 'for', 'it', 'must', 'be', 'told', 'that', 'every', 'other', 'morning', ',', 'when', 'it', 'was', 'felix', \"'\", 's', 'turn', 'to', 'get', 'breakfast', ',', 'felix', 'never', 'was', 'up', 'in', 'decent', 'time', ',', 'and', 'must', 'inevitably', 'have', 'come', 'to', 'public', 'disgrace', 'if', 'franklin', 'had', 'not', 'got', 'all', 'the', 'breakfast', 'things', 'ready', 'for', 'him', ',', 'the', 'bread', 'and', 'butter', 'spread', ',', 'and', 'the', 'toast', 'toasted', ';', 'and', 'had', 'not', ',', 'moreover', ',', 'regularly', ',', 'when', 'the', 'clock', 'struck', 'eight', ',', 'and', 'mrs', '.', 'pomfret', \"'\", 's', 'foot', 'was', 'heard', 'overhead', ',', 'run', 'to', 'call', 'the', 'sleeping', 'felix', ',', 'and', 'helped', 'him', 'constantly', 'through', 'the', 'hurry', 'of', 'getting', 'dressed', 'one', 'instant', 'before', 'the', 'housekeeper', 'came', 'downstairs', '.']\n",
      "\n",
      "\n",
      "Excessive sentence trimmed to: 98 prior to adding <BOS> and <EOS>\n",
      "Special tokens added:\n",
      "[274, 59, 277, 2, 277, 163, 277, 72, 277, 277]...\n",
      "\n",
      "Sentence length after padding: 100\n",
      "Padding tokens added:\n",
      "...[252, 33, 277, 4, 5, 277, 28, 277, 277, 275]\n",
      "\n",
      "Excessive sentence trimmed to: 98 prior to adding <BOS> and <EOS>\n",
      "Special tokens added:\n",
      "[274, 59, 277, 2, 277, 168, 277, 72, 277, 277]...\n",
      "\n",
      "Sentence length after padding: 100\n",
      "Padding tokens added:\n",
      "...[146, 2, 277, 2, 277, 2, 277, 46, 277, 275]\n",
      "\n",
      "Decoder input last 3: tensor([277,  46, 277])\n",
      "Target last 3       : [46, 277, 275]\n",
      "Decoder input size  : 99\n",
      "\n",
      "Labels first 3: tensor([ 59, 277,   2])\n",
      "Target first 3: [274, 59, 277]\n",
      "Labels size   : 99\n",
      "\n",
      "New sample created:\n",
      "    enc_inp size: 100    dec_inp size: 99    labels size : 99\n",
      "    enc_inp type: <class 'torch.Tensor'>\n",
      "    dec_inp type: <class 'torch.Tensor'>\n",
      "    labels  type: <class 'torch.Tensor'>\n",
      "-----------------------------------------------\n",
      "\n",
      "\n",
      "Sentence tokenized!\n",
      "src: ['4', ':', '10', 'philistine', 'fought', ',', 'israel', 'be', 'smite', ',', 'they', 'flee', 'man', 'into', 'his', 'tent', ':', 'there', 'be', 'very', 'great', 'slaughter', ';', 'for', 'there', 'fell', 'of', 'israel', 'thirty', 'thousand', 'footman', '.']\n",
      "tgt: ['4', ':', '10', 'and', 'the', 'philistines', 'fought', ',', 'and', 'israel', 'was', 'smitten', ',', 'and', 'they', 'fled', 'every', 'man', 'into', 'his', 'tent', ':', 'and', 'there', 'was', 'a', 'very', 'great', 'slaughter', ';', 'for', 'there', 'fell', 'of', 'israel', 'thirty', 'thousand', 'footmen', '.']\n",
      "\n",
      "\n",
      "Special tokens added:\n",
      "[274, 204, 37, 277, 277, 277, 2, 277, 28, 277]...\n",
      "\n",
      "Sentence length after padding: 100\n",
      "Padding tokens added:\n",
      "...[276, 276, 276, 276, 276, 276, 276, 276, 276, 276]\n",
      "\n",
      "Special tokens added:\n",
      "[274, 204, 37, 277, 70, 46, 277, 277, 2, 70]...\n",
      "\n",
      "Sentence length after padding: 100\n",
      "Padding tokens added:\n",
      "...[276, 276, 276, 276, 276, 276, 276, 276, 276, 276]\n",
      "\n",
      "Decoder input last 3: tensor([276, 276, 276])\n",
      "Target last 3       : [276, 276, 276]\n",
      "Decoder input size  : 99\n",
      "\n",
      "Labels first 3: tensor([204,  37, 277])\n",
      "Target first 3: [274, 204, 37]\n",
      "Labels size   : 99\n",
      "\n",
      "New sample created:\n",
      "    enc_inp size: 100    dec_inp size: 99    labels size : 99\n",
      "    enc_inp type: <class 'torch.Tensor'>\n",
      "    dec_inp type: <class 'torch.Tensor'>\n",
      "    labels  type: <class 'torch.Tensor'>\n",
      "-----------------------------------------------\n",
      "\n",
      "\n",
      "Sentence tokenized!\n",
      "src: ['2', ':', '14', 'space', 'in', 'which', 'we', 'come', 'from', 'kadeshbarnea', ',', 'until', 'we', 'be', 'come', 'over', 'brook', 'zered', ',', 'be', 'thirty', 'eight', 'year', ';', 'until', 'all', 'generation', 'of', 'men', 'of', 'war', 'be', 'waste', 'out', 'from', 'among', 'host', ',', 'a', 'lord', 'sware', 'unto', 'them', '.']\n",
      "tgt: ['2', ':', '14', 'and', 'the', 'space', 'in', 'which', 'we', 'came', 'from', 'kadeshbarnea', ',', 'until', 'we', 'were', 'come', 'over', 'the', 'brook', 'zered', ',', 'was', 'thirty', 'and', 'eight', 'years', ';', 'until', 'all', 'the', 'generation', 'of', 'the', 'men', 'of', 'war', 'were', 'wasted', 'out', 'from', 'among', 'the', 'host', ',', 'as', 'the', 'lord', 'sware', 'unto', 'them', '.']\n",
      "\n",
      "\n",
      "Special tokens added:\n",
      "[274, 277, 37, 277, 277, 59, 277, 136, 277, 101]...\n",
      "\n",
      "Sentence length after padding: 100\n",
      "Padding tokens added:\n",
      "...[276, 276, 276, 276, 276, 276, 276, 276, 276, 276]\n",
      "\n",
      "Special tokens added:\n",
      "[274, 277, 37, 277, 70, 46, 277, 59, 277, 136]...\n",
      "\n",
      "Sentence length after padding: 100\n",
      "Padding tokens added:\n",
      "...[276, 276, 276, 276, 276, 276, 276, 276, 276, 276]\n",
      "\n",
      "Decoder input last 3: tensor([276, 276, 276])\n",
      "Target last 3       : [276, 276, 276]\n",
      "Decoder input size  : 99\n",
      "\n",
      "Labels first 3: tensor([277,  37, 277])\n",
      "Target first 3: [274, 277, 37]\n",
      "Labels size   : 99\n",
      "\n",
      "New sample created:\n",
      "    enc_inp size: 100    dec_inp size: 99    labels size : 99\n",
      "    enc_inp type: <class 'torch.Tensor'>\n",
      "    dec_inp type: <class 'torch.Tensor'>\n",
      "    labels  type: <class 'torch.Tensor'>\n",
      "-----------------------------------------------\n",
      "\n",
      "\n",
      "Sentence tokenized!\n",
      "src: ['\"', 'lose', 'in', 'desert', 'wild', 'be', 'your', 'little', 'child', '.']\n",
      "tgt: ['\"', 'lost', 'in', 'desert', 'wild', 'is', 'your', 'little', 'child', '.']\n",
      "\n",
      "\n",
      "Special tokens added:\n",
      "[274, 0, 277, 59, 277, 119, 28, 194, 29, 277]...\n",
      "\n",
      "Sentence length after padding: 100\n",
      "Padding tokens added:\n",
      "...[276, 276, 276, 276, 276, 276, 276, 276, 276, 276]\n",
      "\n",
      "Special tokens added:\n",
      "[274, 0, 277, 59, 277, 119, 125, 194, 29, 277]...\n",
      "\n",
      "Sentence length after padding: 100\n",
      "Padding tokens added:\n",
      "...[276, 276, 276, 276, 276, 276, 276, 276, 276, 276]\n",
      "\n",
      "Decoder input last 3: tensor([276, 276, 276])\n",
      "Target last 3       : [276, 276, 276]\n",
      "Decoder input size  : 99\n",
      "\n",
      "Labels first 3: tensor([  0, 277,  59])\n",
      "Target first 3: [274, 0, 277]\n",
      "Labels size   : 99\n",
      "\n",
      "New sample created:\n",
      "    enc_inp size: 100    dec_inp size: 99    labels size : 99\n",
      "    enc_inp type: <class 'torch.Tensor'>\n",
      "    dec_inp type: <class 'torch.Tensor'>\n",
      "    labels  type: <class 'torch.Tensor'>\n",
      "-----------------------------------------------\n",
      "\n",
      "\n",
      "Sentence tokenized!\n",
      "src: ['30', ':', '12', 'wherefore', 'thus', 'saith', 'holy', 'one', 'of', 'israel', ',', 'because', 'ye', 'despise', 'word', ',', 'trust', 'in', 'oppression', 'perverseness', ',', 'stay', 'thereon', ':', '30', ':', '13', 'therefore', 'iniquity', 'shall', 'be', 'you', 'a', 'breach', 'ready', 'fall', ',', 'swell', 'out', 'in', 'high', 'wall', ',', 'whose', 'break', 'cometh', 'suddenly', 'at', 'instant', '.']\n",
      "tgt: ['30', ':', '12', 'wherefore', 'thus', 'saith', 'the', 'holy', 'one', 'of', 'israel', ',', 'because', 'ye', 'despise', 'this', 'word', ',', 'and', 'trust', 'in', 'oppression', 'and', 'perverseness', ',', 'and', 'stay', 'thereon', ':', '30', ':', '13', 'therefore', 'this', 'iniquity', 'shall', 'be', 'to', 'you', 'as', 'a', 'breach', 'ready', 'to', 'fall', ',', 'swelling', 'out', 'in', 'a', 'high', 'wall', ',', 'whose', 'breaking', 'cometh', 'suddenly', 'at', 'an', 'instant', '.']\n",
      "\n",
      "\n",
      "Special tokens added:\n",
      "[274, 277, 37, 277, 277, 277, 277, 277, 145, 41]...\n",
      "\n",
      "Sentence length after padding: 100\n",
      "Padding tokens added:\n",
      "...[276, 276, 276, 276, 276, 276, 276, 276, 276, 276]\n",
      "\n",
      "Special tokens added:\n",
      "[274, 277, 37, 277, 277, 277, 277, 46, 277, 145]...\n",
      "\n",
      "Sentence length after padding: 100\n",
      "Padding tokens added:\n",
      "...[276, 276, 276, 276, 276, 276, 276, 276, 276, 276]\n",
      "\n",
      "Decoder input last 3: tensor([276, 276, 276])\n",
      "Target last 3       : [276, 276, 276]\n",
      "Decoder input size  : 99\n",
      "\n",
      "Labels first 3: tensor([277,  37, 277])\n",
      "Target first 3: [274, 277, 37]\n",
      "Labels size   : 99\n",
      "\n",
      "New sample created:\n",
      "    enc_inp size: 100    dec_inp size: 99    labels size : 99\n",
      "    enc_inp type: <class 'torch.Tensor'>\n",
      "    dec_inp type: <class 'torch.Tensor'>\n",
      "    labels  type: <class 'torch.Tensor'>\n",
      "-----------------------------------------------\n",
      "\n",
      "\n",
      "Sentence tokenized!\n",
      "src: ['mal', '.']\n",
      "tgt: ['mal', '.']\n",
      "\n",
      "\n",
      "Special tokens added:\n",
      "[274, 277, 33, 275]...\n",
      "\n",
      "Sentence length after padding: 100\n",
      "Padding tokens added:\n",
      "...[276, 276, 276, 276, 276, 276, 276, 276, 276, 276]\n",
      "\n",
      "Special tokens added:\n",
      "[274, 277, 33, 275]...\n",
      "\n",
      "Sentence length after padding: 100\n",
      "Padding tokens added:\n",
      "...[276, 276, 276, 276, 276, 276, 276, 276, 276, 276]\n",
      "\n",
      "Decoder input last 3: tensor([276, 276, 276])\n",
      "Target last 3       : [276, 276, 276]\n",
      "Decoder input size  : 99\n",
      "\n",
      "Labels first 3: tensor([277,  33, 275])\n",
      "Target first 3: [274, 277, 33]\n",
      "Labels size   : 99\n",
      "\n",
      "New sample created:\n",
      "    enc_inp size: 100    dec_inp size: 99    labels size : 99\n",
      "    enc_inp type: <class 'torch.Tensor'>\n",
      "    dec_inp type: <class 'torch.Tensor'>\n",
      "    labels  type: <class 'torch.Tensor'>\n",
      "-----------------------------------------------\n",
      "\n",
      "\n",
      "Sentence tokenized!\n",
      "src: ['she', 'be', 'ponder', ',', 'in', 'meanwhile', ',', 'upon', 'possibility', ',', 'without', 'seem', 'very', 'rude', ',', 'of', 'make', 'her', 'escape', 'from', 'jane', 'fairfax', \"'\", 's', 'letter', ',', 'have', 'almost', 'resolve', 'on', 'hurry', 'away', 'directly', 'under', 'slight', 'excuse', ',', 'when', 'miss', 'bates', 'turn', 'her', 'again', 'seize', 'her', 'attention', '.']\n",
      "tgt: ['she', 'was', 'pondering', ',', 'in', 'the', 'meanwhile', ',', 'upon', 'the', 'possibility', ',', 'without', 'seeming', 'very', 'rude', ',', 'of', 'making', 'her', 'escape', 'from', 'jane', 'fairfax', \"'\", 's', 'letter', ',', 'and', 'had', 'almost', 'resolved', 'on', 'hurrying', 'away', 'directly', 'under', 'some', 'slight', 'excuse', ',', 'when', 'miss', 'bates', 'turned', 'to', 'her', 'again', 'and', 'seized', 'her', 'attention', '.']\n",
      "\n",
      "\n",
      "Special tokens added:\n",
      "[274, 19, 28, 277, 2, 59, 277, 2, 277, 277]...\n",
      "\n",
      "Sentence length after padding: 100\n",
      "Padding tokens added:\n",
      "...[276, 276, 276, 276, 276, 276, 276, 276, 276, 276]\n",
      "\n",
      "Special tokens added:\n",
      "[274, 19, 34, 277, 2, 59, 46, 277, 2, 277]...\n",
      "\n",
      "Sentence length after padding: 100\n",
      "Padding tokens added:\n",
      "...[276, 276, 276, 276, 276, 276, 276, 276, 276, 276]\n",
      "\n",
      "Decoder input last 3: tensor([276, 276, 276])\n",
      "Target last 3       : [276, 276, 276]\n",
      "Decoder input size  : 99\n",
      "\n",
      "Labels first 3: tensor([ 19,  34, 277])\n",
      "Target first 3: [274, 19, 34]\n",
      "Labels size   : 99\n",
      "\n",
      "New sample created:\n",
      "    enc_inp size: 100    dec_inp size: 99    labels size : 99\n",
      "    enc_inp type: <class 'torch.Tensor'>\n",
      "    dec_inp type: <class 'torch.Tensor'>\n",
      "    labels  type: <class 'torch.Tensor'>\n",
      "-----------------------------------------------\n",
      "\n",
      "\n",
      "Sentence tokenized!\n",
      "src: ['i', 'be', 'quite', 'ready', '.']\n",
      "tgt: ['i', 'am', 'quite', 'ready', '.']\n",
      "\n",
      "\n",
      "Special tokens added:\n",
      "[274, 52, 28, 226, 277, 33, 275]...\n",
      "\n",
      "Sentence length after padding: 100\n",
      "Padding tokens added:\n",
      "...[276, 276, 276, 276, 276, 276, 276, 276, 276, 276]\n",
      "\n",
      "Special tokens added:\n",
      "[274, 52, 277, 226, 277, 33, 275]...\n",
      "\n",
      "Sentence length after padding: 100\n",
      "Padding tokens added:\n",
      "...[276, 276, 276, 276, 276, 276, 276, 276, 276, 276]\n",
      "\n",
      "Decoder input last 3: tensor([276, 276, 276])\n",
      "Target last 3       : [276, 276, 276]\n",
      "Decoder input size  : 99\n",
      "\n",
      "Labels first 3: tensor([ 52, 277, 226])\n",
      "Target first 3: [274, 52, 277]\n",
      "Labels size   : 99\n",
      "\n",
      "New sample created:\n",
      "    enc_inp size: 100    dec_inp size: 99    labels size : 99\n",
      "    enc_inp type: <class 'torch.Tensor'>\n",
      "    dec_inp type: <class 'torch.Tensor'>\n",
      "    labels  type: <class 'torch.Tensor'>\n",
      "-----------------------------------------------\n",
      "\n",
      "\n",
      "Sentence tokenized!\n",
      "src: ['be', ',', 'perhaps', ',', 'one', 'of', 'happy', 'day', 'of', 'your', 'life', '.\"']\n",
      "tgt: ['this', 'is', ',', 'perhaps', ',', 'one', 'of', 'the', 'happiest', 'days', 'of', 'your', 'life', '.\"']\n",
      "\n",
      "\n",
      "Special tokens added:\n",
      "[274, 28, 2, 277, 2, 145, 41, 216, 113, 41]...\n",
      "\n",
      "Sentence length after padding: 100\n",
      "Padding tokens added:\n",
      "...[276, 276, 276, 276, 276, 276, 276, 276, 276, 276]\n",
      "\n",
      "Special tokens added:\n",
      "[274, 152, 125, 2, 277, 2, 145, 41, 46, 277]...\n",
      "\n",
      "Sentence length after padding: 100\n",
      "Padding tokens added:\n",
      "...[276, 276, 276, 276, 276, 276, 276, 276, 276, 276]\n",
      "\n",
      "Decoder input last 3: tensor([276, 276, 276])\n",
      "Target last 3       : [276, 276, 276]\n",
      "Decoder input size  : 99\n",
      "\n",
      "Labels first 3: tensor([152, 125,   2])\n",
      "Target first 3: [274, 152, 125]\n",
      "Labels size   : 99\n",
      "\n",
      "New sample created:\n",
      "    enc_inp size: 100    dec_inp size: 99    labels size : 99\n",
      "    enc_inp type: <class 'torch.Tensor'>\n",
      "    dec_inp type: <class 'torch.Tensor'>\n",
      "    labels  type: <class 'torch.Tensor'>\n",
      "-----------------------------------------------\n",
      "\n",
      "\n",
      "Sentence tokenized!\n",
      "src: ['18', ':', '9', 'king', 'of', 'israel', 'jehoshaphat', 'king', 'of', 'judah', 'sit', 'of', 'them', 'on', 'his', 'throne', ',', 'clothe', 'in', 'their', 'robe', ',', 'they', 'sit', 'in', 'void', 'place', 'at', 'enter', 'in', 'of', 'gate', 'of', 'samaria', ';', 'all', 'prophet', 'prophesy', 'before', 'them', '.']\n",
      "tgt: ['18', ':', '9', 'and', 'the', 'king', 'of', 'israel', 'and', 'jehoshaphat', 'king', 'of', 'judah', 'sat', 'either', 'of', 'them', 'on', 'his', 'throne', ',', 'clothed', 'in', 'their', 'robes', ',', 'and', 'they', 'sat', 'in', 'a', 'void', 'place', 'at', 'the', 'entering', 'in', 'of', 'the', 'gate', 'of', 'samaria', ';', 'and', 'all', 'the', 'prophets', 'prophesied', 'before', 'them', '.']\n",
      "\n",
      "\n",
      "Special tokens added:\n",
      "[274, 259, 37, 277, 277, 41, 277, 277, 277, 41]...\n",
      "\n",
      "Sentence length after padding: 100\n",
      "Padding tokens added:\n",
      "...[276, 276, 276, 276, 276, 276, 276, 276, 276, 276]\n",
      "\n",
      "Special tokens added:\n",
      "[274, 259, 37, 277, 70, 46, 277, 41, 277, 70]...\n",
      "\n",
      "Sentence length after padding: 100\n",
      "Padding tokens added:\n",
      "...[276, 276, 276, 276, 276, 276, 276, 276, 276, 276]\n",
      "\n",
      "Decoder input last 3: tensor([276, 276, 276])\n",
      "Target last 3       : [276, 276, 276]\n",
      "Decoder input size  : 99\n",
      "\n",
      "Labels first 3: tensor([259,  37, 277])\n",
      "Target first 3: [274, 259, 37]\n",
      "Labels size   : 99\n",
      "\n",
      "New sample created:\n",
      "    enc_inp size: 100    dec_inp size: 99    labels size : 99\n",
      "    enc_inp type: <class 'torch.Tensor'>\n",
      "    dec_inp type: <class 'torch.Tensor'>\n",
      "    labels  type: <class 'torch.Tensor'>\n",
      "-----------------------------------------------\n",
      "\n",
      "\n",
      "Sentence tokenized!\n",
      "src: ['selah', '.']\n",
      "tgt: ['selah', '.']\n",
      "\n",
      "\n",
      "Special tokens added:\n",
      "[274, 277, 33, 275]...\n",
      "\n",
      "Sentence length after padding: 100\n",
      "Padding tokens added:\n",
      "...[276, 276, 276, 276, 276, 276, 276, 276, 276, 276]\n",
      "\n",
      "Special tokens added:\n",
      "[274, 277, 33, 275]...\n",
      "\n",
      "Sentence length after padding: 100\n",
      "Padding tokens added:\n",
      "...[276, 276, 276, 276, 276, 276, 276, 276, 276, 276]\n",
      "\n",
      "Decoder input last 3: tensor([276, 276, 276])\n",
      "Target last 3       : [276, 276, 276]\n",
      "Decoder input size  : 99\n",
      "\n",
      "Labels first 3: tensor([277,  33, 275])\n",
      "Target first 3: [274, 277, 33]\n",
      "Labels size   : 99\n",
      "\n",
      "New sample created:\n",
      "    enc_inp size: 100    dec_inp size: 99    labels size : 99\n",
      "    enc_inp type: <class 'torch.Tensor'>\n",
      "    dec_inp type: <class 'torch.Tensor'>\n",
      "    labels  type: <class 'torch.Tensor'>\n",
      "-----------------------------------------------\n",
      "\n",
      "\n",
      "Sentence tokenized!\n",
      "src: ['22', ':', '27', 'last', 'of', 'all', 'woman', 'die', 'also', '.']\n",
      "tgt: ['22', ':', '27', 'and', 'last', 'of', 'all', 'the', 'woman', 'died', 'also', '.']\n",
      "\n",
      "\n",
      "Special tokens added:\n",
      "[274, 277, 37, 277, 250, 41, 86, 50, 164, 277]...\n",
      "\n",
      "Sentence length after padding: 100\n",
      "Padding tokens added:\n",
      "...[276, 276, 276, 276, 276, 276, 276, 276, 276, 276]\n",
      "\n",
      "Special tokens added:\n",
      "[274, 277, 37, 277, 70, 250, 41, 86, 46, 50]...\n",
      "\n",
      "Sentence length after padding: 100\n",
      "Padding tokens added:\n",
      "...[276, 276, 276, 276, 276, 276, 276, 276, 276, 276]\n",
      "\n",
      "Decoder input last 3: tensor([276, 276, 276])\n",
      "Target last 3       : [276, 276, 276]\n",
      "Decoder input size  : 99\n",
      "\n",
      "Labels first 3: tensor([277,  37, 277])\n",
      "Target first 3: [274, 277, 37]\n",
      "Labels size   : 99\n",
      "\n",
      "New sample created:\n",
      "    enc_inp size: 100    dec_inp size: 99    labels size : 99\n",
      "    enc_inp type: <class 'torch.Tensor'>\n",
      "    dec_inp type: <class 'torch.Tensor'>\n",
      "    labels  type: <class 'torch.Tensor'>\n",
      "-----------------------------------------------\n",
      "\n",
      "\n",
      "Sentence tokenized!\n",
      "src: ['\"', 'pan', 'again', '!\"']\n",
      "tgt: ['\"', 'pan', 'again', '!\"']\n",
      "\n",
      "\n",
      "Special tokens added:\n",
      "[274, 0, 277, 155, 277, 275]...\n",
      "\n",
      "Sentence length after padding: 100\n",
      "Padding tokens added:\n",
      "...[276, 276, 276, 276, 276, 276, 276, 276, 276, 276]\n",
      "\n",
      "Special tokens added:\n",
      "[274, 0, 277, 155, 277, 275]...\n",
      "\n",
      "Sentence length after padding: 100\n",
      "Padding tokens added:\n",
      "...[276, 276, 276, 276, 276, 276, 276, 276, 276, 276]\n",
      "\n",
      "Decoder input last 3: tensor([276, 276, 276])\n",
      "Target last 3       : [276, 276, 276]\n",
      "Decoder input size  : 99\n",
      "\n",
      "Labels first 3: tensor([  0, 277, 155])\n",
      "Target first 3: [274, 0, 277]\n",
      "Labels size   : 99\n",
      "\n",
      "New sample created:\n",
      "    enc_inp size: 100    dec_inp size: 99    labels size : 99\n",
      "    enc_inp type: <class 'torch.Tensor'>\n",
      "    dec_inp type: <class 'torch.Tensor'>\n",
      "    labels  type: <class 'torch.Tensor'>\n",
      "-----------------------------------------------\n",
      "\n",
      "\n",
      "Sentence tokenized!\n",
      "src: ['i', 'should', 'not', 'be', 'at', 'surprised', 'if', ',', 'when', 'you', 'count', 'scientific', 'investigation', 'discovery', 'since', 'fall', 'of', 'rome', ',', 'you', 'find', 'that', 'great', 'mass', 'of', 'them', 'have', 'be', 'make', 'by', 'monk', '.']\n",
      "tgt: ['i', 'should', 'not', 'be', 'at', 'all', 'surprised', 'if', ',', 'when', 'you', 'counted', 'the', 'scientific', 'investigations', 'and', 'discoveries', 'since', 'the', 'fall', 'of', 'rome', ',', 'you', 'found', 'that', 'a', 'great', 'mass', 'of', 'them', 'had', 'been', 'made', 'by', 'monks', '.']\n",
      "\n",
      "\n",
      "Special tokens added:\n",
      "[274, 52, 277, 146, 28, 106, 277, 181, 2, 277]...\n",
      "\n",
      "Sentence length after padding: 100\n",
      "Padding tokens added:\n",
      "...[276, 276, 276, 276, 276, 276, 276, 276, 276, 276]\n",
      "\n",
      "Special tokens added:\n",
      "[274, 52, 277, 146, 28, 106, 86, 277, 181, 2]...\n",
      "\n",
      "Sentence length after padding: 100\n",
      "Padding tokens added:\n",
      "...[276, 276, 276, 276, 276, 276, 276, 276, 276, 276]\n",
      "\n",
      "Decoder input last 3: tensor([276, 276, 276])\n",
      "Target last 3       : [276, 276, 276]\n",
      "Decoder input size  : 99\n",
      "\n",
      "Labels first 3: tensor([ 52, 277, 146])\n",
      "Target first 3: [274, 52, 277]\n",
      "Labels size   : 99\n",
      "\n",
      "New sample created:\n",
      "    enc_inp size: 100    dec_inp size: 99    labels size : 99\n",
      "    enc_inp type: <class 'torch.Tensor'>\n",
      "    dec_inp type: <class 'torch.Tensor'>\n",
      "    labels  type: <class 'torch.Tensor'>\n",
      "-----------------------------------------------\n",
      "\n",
      "\n",
      "Sentence tokenized!\n",
      "src: ['\"', 'oh', '!']\n",
      "tgt: ['\"', 'oh', '!']\n",
      "\n",
      "\n",
      "Special tokens added:\n",
      "[274, 0, 277, 277, 275]...\n",
      "\n",
      "Sentence length after padding: 100\n",
      "Padding tokens added:\n",
      "...[276, 276, 276, 276, 276, 276, 276, 276, 276, 276]\n",
      "\n",
      "Special tokens added:\n",
      "[274, 0, 277, 277, 275]...\n",
      "\n",
      "Sentence length after padding: 100\n",
      "Padding tokens added:\n",
      "...[276, 276, 276, 276, 276, 276, 276, 276, 276, 276]\n",
      "\n",
      "Decoder input last 3: tensor([276, 276, 276])\n",
      "Target last 3       : [276, 276, 276]\n",
      "Decoder input size  : 99\n",
      "\n",
      "Labels first 3: tensor([  0, 277, 277])\n",
      "Target first 3: [274, 0, 277]\n",
      "Labels size   : 99\n",
      "\n",
      "New sample created:\n",
      "    enc_inp size: 100    dec_inp size: 99    labels size : 99\n",
      "    enc_inp type: <class 'torch.Tensor'>\n",
      "    dec_inp type: <class 'torch.Tensor'>\n",
      "    labels  type: <class 'torch.Tensor'>\n",
      "-----------------------------------------------\n",
      "\n",
      "\n",
      "Sentence tokenized!\n",
      "src: ['\"', 'god', 'bless', 'my', 'soul', '!']\n",
      "tgt: ['\"', 'god', 'bless', 'my', 'soul', '!']\n",
      "\n",
      "\n",
      "Special tokens added:\n",
      "[274, 0, 277, 277, 43, 207, 277, 275]...\n",
      "\n",
      "Sentence length after padding: 100\n",
      "Padding tokens added:\n",
      "...[276, 276, 276, 276, 276, 276, 276, 276, 276, 276]\n",
      "\n",
      "Special tokens added:\n",
      "[274, 0, 277, 277, 43, 207, 277, 275]...\n",
      "\n",
      "Sentence length after padding: 100\n",
      "Padding tokens added:\n",
      "...[276, 276, 276, 276, 276, 276, 276, 276, 276, 276]\n",
      "\n",
      "Decoder input last 3: tensor([276, 276, 276])\n",
      "Target last 3       : [276, 276, 276]\n",
      "Decoder input size  : 99\n",
      "\n",
      "Labels first 3: tensor([  0, 277, 277])\n",
      "Target first 3: [274, 0, 277]\n",
      "Labels size   : 99\n",
      "\n",
      "New sample created:\n",
      "    enc_inp size: 100    dec_inp size: 99    labels size : 99\n",
      "    enc_inp type: <class 'torch.Tensor'>\n",
      "    dec_inp type: <class 'torch.Tensor'>\n",
      "    labels  type: <class 'torch.Tensor'>\n",
      "-----------------------------------------------\n",
      "\n",
      "\n",
      "Sentence tokenized!\n",
      "src: ['still', 'more', 'strange', 'see', 'him', 'giddily', 'perch', 'upon', 'loggerhead', 'itself', ',', 'under', 'such', 'circumstance', '.']\n",
      "tgt: ['still', 'more', 'strange', 'to', 'see', 'him', 'giddily', 'perched', 'upon', 'the', 'loggerhead', 'itself', ',', 'under', 'such', 'circumstances', '.']\n",
      "\n",
      "\n",
      "Special tokens added:\n",
      "[274, 277, 277, 277, 277, 137, 277, 277, 277, 277]...\n",
      "\n",
      "Sentence length after padding: 100\n",
      "Padding tokens added:\n",
      "...[276, 276, 276, 276, 276, 276, 276, 276, 276, 276]\n",
      "\n",
      "Special tokens added:\n",
      "[274, 277, 277, 277, 35, 277, 137, 277, 277, 277]...\n",
      "\n",
      "Sentence length after padding: 100\n",
      "Padding tokens added:\n",
      "...[276, 276, 276, 276, 276, 276, 276, 276, 276, 276]\n",
      "\n",
      "Decoder input last 3: tensor([276, 276, 276])\n",
      "Target last 3       : [276, 276, 276]\n",
      "Decoder input size  : 99\n",
      "\n",
      "Labels first 3: tensor([277, 277, 277])\n",
      "Target first 3: [274, 277, 277]\n",
      "Labels size   : 99\n",
      "\n",
      "New sample created:\n",
      "    enc_inp size: 100    dec_inp size: 99    labels size : 99\n",
      "    enc_inp type: <class 'torch.Tensor'>\n",
      "    dec_inp type: <class 'torch.Tensor'>\n",
      "    labels  type: <class 'torch.Tensor'>\n",
      "-----------------------------------------------\n",
      "\n",
      "\n",
      "Sentence tokenized!\n",
      "src: ['26', ':', '50', 'be', 'family', 'of', 'naphtali', 'accord', 'their', 'family', ':', 'they', 'that', 'be', 'number', 'of', 'them', 'be', 'forty', 'five', 'thousand', 'four', 'hundred', '.']\n",
      "tgt: ['26', ':', '50', 'these', 'are', 'the', 'families', 'of', 'naphtali', 'according', 'to', 'their', 'families', ':', 'and', 'they', 'that', 'were', 'numbered', 'of', 'them', 'were', 'forty', 'and', 'five', 'thousand', 'and', 'four', 'hundred', '.']\n",
      "\n",
      "\n",
      "Special tokens added:\n",
      "[274, 277, 37, 277, 28, 277, 41, 277, 277, 277]...\n",
      "\n",
      "Sentence length after padding: 100\n",
      "Padding tokens added:\n",
      "...[276, 276, 276, 276, 276, 276, 276, 276, 276, 276]\n",
      "\n",
      "Special tokens added:\n",
      "[274, 277, 37, 277, 277, 150, 46, 277, 41, 277]...\n",
      "\n",
      "Sentence length after padding: 100\n",
      "Padding tokens added:\n",
      "...[276, 276, 276, 276, 276, 276, 276, 276, 276, 276]\n",
      "\n",
      "Decoder input last 3: tensor([276, 276, 276])\n",
      "Target last 3       : [276, 276, 276]\n",
      "Decoder input size  : 99\n",
      "\n",
      "Labels first 3: tensor([277,  37, 277])\n",
      "Target first 3: [274, 277, 37]\n",
      "Labels size   : 99\n",
      "\n",
      "New sample created:\n",
      "    enc_inp size: 100    dec_inp size: 99    labels size : 99\n",
      "    enc_inp type: <class 'torch.Tensor'>\n",
      "    dec_inp type: <class 'torch.Tensor'>\n",
      "    labels  type: <class 'torch.Tensor'>\n",
      "-----------------------------------------------\n",
      "\n",
      "\n",
      "Sentence tokenized!\n",
      "src: ['18', ':', '26', 'then', 'say', 'eliakim', 'son', 'of', 'hilkiah', ',', 'shebna', ',', 'joah', ',', 'unto', 'rabshakeh', ',', 'speak', ',', 'i', 'pray', 'thee', ',', 'thy', 'servant', 'in', 'syrian', 'language', ';', 'for', 'we', 'understand', 'it', ':', 'talk', 'not', 'with', 'u', 'in', 'jew', \"'\", 'language', 'in', 'ear', 'of', 'people', 'that', 'be', 'on', 'wall', '.']\n",
      "tgt: ['18', ':', '26', 'then', 'said', 'eliakim', 'the', 'son', 'of', 'hilkiah', ',', 'and', 'shebna', ',', 'and', 'joah', ',', 'unto', 'rabshakeh', ',', 'speak', ',', 'i', 'pray', 'thee', ',', 'to', 'thy', 'servants', 'in', 'the', 'syrian', 'language', ';', 'for', 'we', 'understand', 'it', ':', 'and', 'talk', 'not', 'with', 'us', 'in', 'the', 'jews', \"'\", 'language', 'in', 'the', 'ears', 'of', 'the', 'people', 'that', 'are', 'on', 'the', 'wall', '.']\n",
      "\n",
      "\n",
      "Special tokens added:\n",
      "[274, 259, 37, 277, 174, 13, 277, 142, 41, 277]...\n",
      "\n",
      "Sentence length after padding: 100\n",
      "Padding tokens added:\n",
      "...[276, 276, 276, 276, 276, 276, 276, 276, 276, 276]\n",
      "\n",
      "Special tokens added:\n",
      "[274, 259, 37, 277, 174, 24, 277, 46, 142, 41]...\n",
      "\n",
      "Sentence length after padding: 100\n",
      "Padding tokens added:\n",
      "...[276, 276, 276, 276, 276, 276, 276, 276, 276, 276]\n",
      "\n",
      "Decoder input last 3: tensor([276, 276, 276])\n",
      "Target last 3       : [276, 276, 276]\n",
      "Decoder input size  : 99\n",
      "\n",
      "Labels first 3: tensor([259,  37, 277])\n",
      "Target first 3: [274, 259, 37]\n",
      "Labels size   : 99\n",
      "\n",
      "New sample created:\n",
      "    enc_inp size: 100    dec_inp size: 99    labels size : 99\n",
      "    enc_inp type: <class 'torch.Tensor'>\n",
      "    dec_inp type: <class 'torch.Tensor'>\n",
      "    labels  type: <class 'torch.Tensor'>\n",
      "-----------------------------------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ts_data = SeqPairDataset(\"data/toy2.json\", tokenizer, MAX_LEN, MAX_LEN)\n",
    "testloader = DataLoader(ts_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3f6ce776",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BLEU Score Calculation:   4%|▍         | 1/25 [00:00<00:08,  3.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0: sample output:PREDICTED: ['and', '<unk>', 'and', 'a', 'and', '<unk>', 'we', '<unk>', '<unk>', 'and', ';', '<unk>', 'engedi', '<unk>', '<unk>', '<unk>', 'and', '<unk>', 'and', '<unk>', '<unk>', 'and', '<unk>', 'and', ';', '<unk>', 'we', '<unk>', 'we', '<unk>', 'we', '<unk>', 'and', '<unk>', 'we', '<unk>', 'and', ';', 'by', ',', 'and', ';', '<unk>', 'and', ';', '<unk>', 'we', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', 'we', 'for', '<unk>', 'and', 'often', 'jared', '<unk>', '-', '<unk>', '<unk>', '<unk>', 'and', '<unk>', 'engedi', 'for', '<unk>', 'and', '<unk>', '<unk>', '<unk>', 'we', 'yours', '<unk>', 'and', '<unk>', 'we', '<unk>', 'if', '<unk>', 'we', '<unk>', '<unk>', '<unk>', 'for', '<unk>', 'and', ';', '<unk>', 'engedi', '<unk>', 'own', '<unk>', '<unk>', '<unk>', 'we', '<unk>', 'we']\n",
      "ACTUAL:    ['you', '<unk>', '<unk>', '<unk>']\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BLEU Score Calculation:  44%|████▍     | 11/25 [00:03<00:04,  3.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 10: sample output:PREDICTED: ['<unk>', '<unk>', 'and', '<unk>', 'and', '<unk>', 'engedi', '<unk>', '<unk>', 'and', '<unk>', 'enoch', 'somehow', 'unto', '<unk>', '<unk>', 'and', '<unk>', '<unk>', '<unk>', '<unk>', 'and', 's', '19', '<unk>', 'and', '<unk>', 'we', '<unk>', 'engedi', '<unk>', '<unk>', 'we', '<unk>', 'engedi', '<unk>', 'and', 'her', 'and', '<unk>', 'and', ';', '<unk>', 'we', '<unk>', 'engedi', 'you', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', 'we', '<unk>', 'for', '<unk>', 'engedi', '<unk>', 'for', 'writer', '<unk>', '<unk>', '<unk>', 'and', '<unk>', 'engedi', '<unk>', 'we', '<unk>', '<unk>', '<unk>', '<unk>', 'we', 'yours', '<unk>', 'and', '<unk>', 'we', '<unk>', 'if', '<unk>', 'we', '<unk>', '<unk>', '<unk>', 'after', '<unk>', 'and', ';', '<unk>', 'engedi', '<unk>', 'own', '<unk>', '<unk>', '<unk>', 'we', '<unk>', 'we']\n",
      "ACTUAL:    ['<unk>', ':', '<unk>', '<unk>', '<unk>', '<unk>', 'the', '<unk>', 'one', 'of', '<unk>', ',', '<unk>', '<unk>', '<unk>', 'this', '<unk>', ',', 'and', '<unk>', 'in', '<unk>', 'and', '<unk>', ',', 'and', '<unk>', '<unk>', ':', '<unk>', ':', '<unk>', '<unk>', 'this', '<unk>', 'shall', 'be', 'to', 'you', 'as', 'a', '<unk>', '<unk>', 'to', '<unk>', ',', '<unk>', 'out', 'in', 'a', 'high', '<unk>', ',', '<unk>', '<unk>', '<unk>', '<unk>', 'at', 'an', '<unk>', '.']\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BLEU Score Calculation:  84%|████████▍ | 21/25 [00:06<00:01,  3.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 20: sample output:PREDICTED: ['and', '<unk>', 'and', 'a', 'and', '<unk>', 'we', '<unk>', 'we', 'for', '<unk>', 'enoch', 'somehow', 'unto', '<unk>', '<unk>', 'and', '<unk>', 'and', '<unk>', 'we', '<unk>', 'yours', '<unk>', 'and', 'son', ',', 'and', ';', '<unk>', 'we', '<unk>', 'we', '<unk>', 'we', '<unk>', 'and', ';', 'by', ',', 'and', ';', '<unk>', 'we', '<unk>', 'we', 'for', '<unk>', 'by', '<unk>', '<unk>', '<unk>', 'we', 'for', '<unk>', 'and', 'often', 'to', '<unk>', '-', '<unk>', '<unk>', '<unk>', 'and', '<unk>', 'engedi', 'for', '<unk>', 'if', '<unk>', '<unk>', 'we', '<unk>', 'we', '<unk>', 'and', '<unk>', 'we', '<unk>', 'if', '<unk>', 'we', '<unk>', '<unk>', '<unk>', 'for', '<unk>', 'and', ';', '<unk>', 'engedi', '<unk>', 'own', '<unk>', '<unk>', '<unk>', 'we', '<unk>', 'we']\n",
      "ACTUAL:    ['\"', '<unk>', '<unk>']\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BLEU Score Calculation: 100%|██████████| 25/25 [00:07<00:00,  3.26it/s]\n"
     ]
    }
   ],
   "source": [
    "bleu_avg = main.compute_bleu_score(model, testloader, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b7070982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bos_id: 274\n",
      "eos_id: 275\n",
      "src_vocab_size: 278\n",
      "tgt_vocab_size: 278\n"
     ]
    }
   ],
   "source": [
    "print('bos_id:', tokenizer.bos_id)\n",
    "print('eos_id:', tokenizer.eos_id)\n",
    "print('src_vocab_size:', len(tokenizer.src_vocab))\n",
    "print('tgt_vocab_size:', len(tokenizer.tgt_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8fcc8f06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14136969696969695"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bleu_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fa0d7480",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Trainable Parameters: 1,058,838\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    # Iterate through all parameters returned by model.parameters()\n",
    "    # Check if p.requires_grad is True (meaning it is trainable)\n",
    "    # Sum the number of elements (numel) in each trainable tensor (weight or bias)\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "# Example usage in your main.py:\n",
    "total_params = count_parameters(model)\n",
    "print(f\"Total Trainable Parameters: {total_params:,}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project-2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
